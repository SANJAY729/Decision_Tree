{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SANJAY729/Decision_Tree/blob/master/allo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnYS2roA3MI",
        "outputId": "0cd14f61-43bb-4059-ee6a-12fdeabae367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"allo testing dhar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allo testing dhar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwmH98velv8"
      },
      "source": [
        "#hello123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBCiLs7wXbBP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aitjxFhv7p"
      },
      "source": [
        "#The Pog Code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "################### \n",
        "    \n",
        "###################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,max_depth,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    # print(features)\n",
        "    # print(data[target_attribute_name])\n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) == 0 or max_depth == 0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{},\"positive\":0,\"negative\":0}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,max_depth-1,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    default = \"no-recurrence-events\"\n",
        "    if (tree[\"positive\"]>tree[\"negative\"]):\n",
        "        default = \"recurrence-events\"\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree) \n",
        "    return (np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100\n",
        "    #print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
        "    \n",
        "def majorityclassutil(query,tree):\n",
        "    if (query[\"class\"]==\"no-recurrence-events\"):\n",
        "        tree[\"negative\"] = tree[\"negative\"] + 1\n",
        "    else:\n",
        "        tree[\"positive\"] = tree[\"positive\"] + 1\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return\n",
        "            result = tree[key][query[key]]\n",
        "            if isinstance(result,dict):\n",
        "                return majorityclassutil(query,result)\n",
        "            else:\n",
        "                return\n",
        "def majorityclasses(data,tree):\n",
        "     queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "     for i in range(len(data)):\n",
        "        majorityclassutil(queries[i],tree)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PANbgbMCUBZ"
      },
      "source": [
        "#Import the dataset and define the feature as well as the target datasets / columns#\n",
        "dataset = pd.read_csv('breast-cancer.csv',\n",
        "                      names=['class','age','menopause','tumor-size','inv-nodes','node-caps','def-malig','breast','breast-quad','irradiat'])#Import all columns omitting the fist which consists the names of the animals\n",
        "# Class: no-recurrence-events, recurrence-events\n",
        "# 2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
        "# 3. menopause: lt40, ge40, premeno.\n",
        "# 4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
        "# 5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
        "# 6. node-caps: yes, no.\n",
        "# 7. deg-malig: 1, 2, 3.\n",
        "# 8. breast: left, right.\n",
        "# 9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
        "# 10. irradiat: yes, no.\n",
        "\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "#dataset=dataset.drop('animal_name',axis=1)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j9scP8RCbVe"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:230].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,testing_data"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uPcELuCG4Z",
        "outputId": "7d9f1e7a-fdcf-4318-e2ca-5eca3ed6dfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "x = [1,2,3,4,5]\n",
        "y = []\n",
        "for i in range (1,6):\n",
        "    accuracy = 0\n",
        "    for j in range(1,11):\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "        training_data,testing_data=train_test_split(dataset)\n",
        "        tree = ID3(training_data,training_data,training_data.columns[1:],i)\n",
        "        majorityclasses(training_data,tree)\n",
        "        accuracy = accuracy + test(testing_data,tree)\n",
        "    y.append(accuracy/10)\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKnuaJ_S1dew",
        "outputId": "b226b7aa-44ef-42cd-db86-2a0f2b4251e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\"\"\"\n",
        "#with sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "x = [1,2,3,4,5]\n",
        "y = []\n",
        "for i in range (1,6):\n",
        "    accuracy = 0\n",
        "    for j in range(1,11):\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "        training_data,testing_data=train_test_split(dataset)\n",
        "        x_train = training_data.iloc[0:230,1:10]\n",
        "        y_train = training_data.iloc[0:230,0:1]\n",
        "        x_test  = testing_data.iloc[0:230,1:10]\n",
        "        y_test  = testing_data.iloc[0:230,0:1]\n",
        "        clf = tree.DecisionTreeClassifier(criterion = \"entropy\", max_depth = i)\n",
        "        clf.fit(x_train,y_train)\n",
        "        accuracy = accuracy + clf.score(x_train,y_train)\n",
        "    y.append(accuracy/10)\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show\n",
        "\"\"\""
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#with sklearn\\nfrom sklearn.tree import DecisionTreeClassifier \\nx = [1,2,3,4,5]\\ny = []\\nfor i in range (1,6):\\n    accuracy = 0\\n    for j in range(1,11):\\n        dataset = dataset.sample(frac=1).reset_index(drop=True)\\n        training_data,testing_data=train_test_split(dataset)\\n        x_train = training_data.iloc[0:230,1:10]\\n        y_train = training_data.iloc[0:230,0:1]\\n        x_test  = testing_data.iloc[0:230,1:10]\\n        y_test  = testing_data.iloc[0:230,0:1]\\n        clf = tree.DecisionTreeClassifier(criterion = \"entropy\", max_depth = i)\\n        clf.fit(x_train,y_train)\\n        accuracy = accuracy + clf.score(x_train,y_train)\\n    y.append(accuracy/10)\\nplt.plot(x,y)\\nplt.ylabel(\\'Accuracy\\')\\nplt.xlabel(\\'Max Depth\\')\\nplt.show\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl6UzIF4qQOD"
      },
      "source": [
        "#pruning\n",
        "def train_validation_test_split(dataset):\n",
        "    training_data = dataset.iloc[:180].reset_index(drop=True)\n",
        "    validation_data = dataset.iloc[180:230].reset_index(drop=True)\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,validation_data,testing_data\n",
        "\n",
        "max_depth  = 4\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "training_data,validation_data,testing_data = train_validation_test_split(dataset)\n",
        "tree = ID3(training_data,training_data,training_data.columns[1:],max_depth)\n",
        "initial_accuracy = test(validation_data,tree)\n",
        "\n",
        "#def post_pruning(tree,initial_accuracy,validation_data):\n"
      ],
      "execution_count": 51,
      "outputs": []
    }
  ]
}