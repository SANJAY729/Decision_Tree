{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SANJAY729/Decision_Tree/blob/master/allo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnYS2roA3MI",
        "outputId": "0cd14f61-43bb-4059-ee6a-12fdeabae367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"allo testing dhar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allo testing dhar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwmH98velv8"
      },
      "source": [
        "#hello123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBCiLs7wXbBP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aitjxFhv7p"
      },
      "source": [
        "#The Pog Code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "################### \n",
        "    \n",
        "###################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,max_depth,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    # print(features)\n",
        "    # print(data[target_attribute_name])\n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) == 0 or max_depth == 0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,max_depth-1,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree,default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    #1.\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    return (np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100\n",
        "    #print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PANbgbMCUBZ"
      },
      "source": [
        "#Import the dataset and define the feature as well as the target datasets / columns#\n",
        "dataset = pd.read_csv('breast-cancer.csv',\n",
        "                      names=['class','age','menopause','tumor-size','inv-nodes','node-caps','def-malig','breast','breast-quad','irradiat'])#Import all columns omitting the fist which consists the names of the animals\n",
        "# Class: no-recurrence-events, recurrence-events\n",
        "# 2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
        "# 3. menopause: lt40, ge40, premeno.\n",
        "# 4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
        "# 5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
        "# 6. node-caps: yes, no.\n",
        "# 7. deg-malig: 1, 2, 3.\n",
        "# 8. breast: left, right.\n",
        "# 9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
        "# 10. irradiat: yes, no.\n",
        "\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "#dataset=dataset.drop('animal_name',axis=1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j9scP8RCbVe"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:230].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,testing_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uPcELuCG4Z",
        "outputId": "0e70cf00-cfd9-4167-bb23-167b16defb97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "x = [2,3,4,5]\n",
        "y = []\n",
        "for i in range (2,6):\n",
        "    accuracy = 0\n",
        "    for j in range(1,11):\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "        training_data,testing_data=train_test_split(dataset)\n",
        "        tree = ID3(training_data,training_data,training_data.columns[1:],i)\n",
        "        accuracy = accuracy + test(training_data,tree)\n",
        "    y.append(accuracy/10)\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfrG8e8DofcuLVRFqpQRsNe1Cyq6gn1XFnB1Lbtr23Xt66qrK3bFtq5dARU71a5IQgeBhE5ooYTekjy/P+bw2yxOYIA5mZT7c125kjnnvGeew4S5c953znvM3REREdlTuWQXICIixZMCQkREYlJAiIhITAoIERGJSQEhIiIxKSBERCSmUAPCzG4ws5lmNsvMbgyW3W1mWWY2Nfg6q5C2Z5jZXDPLNLPbwqxTRER+ycK6DsLMOgFvAz2BncDnwBDgMmCzuz+yl7blgXnAr4BlwCRggLvPDqVYERH5hZQQ990emOjuWwHM7Cvggjjb9gQy3X1B0PZtoC+w14CoX7++t2zZ8oALFhEpa9LT09e4e4NY68IMiJnA382sHrANOAtIA9YC15nZFcHjP7n7+j3aNgWWFni8DOgV60nMbBAwCCA1NZW0tLSEHoSISGlmZosLWxfaGIS7/ww8BIwm2r00FcgDngXaAF2BFcCjB/k8w9w94u6RBg1ihqCIiByAUAep3f0ld+/h7scD64F57r7K3fPcPR94gWh30p6ygOYFHjcLlomISBEJ+1NMDYPvqUTHH940s8YFNjmfaFfUniYBh5pZKzOrCPQHRoVZq4iI/K8wxyAARgRjELuAa909x8yeNLOugAOLgMEAZtYEeNHdz3L3XDO7DvgCKA+87O6zQq5VREQKCDUg3P24GMsuL2Tb5UQHsnc//hT4NLzqRERkb3QltYiIxKSAEBGRmBQQIiIl2KRF63juq/mh7DvsQWoREQnBpu27ePjzubz242JS61bliqNaULViYt/SFRAiIiXMhDmr+ev7M1ixcTu/PaYVfzrtsISHAyggRERKjHVbdnLvR7P4YOpyDm1YnRHXHE331DqhPZ8CQkSkmHN3Rk1bzj0fzWbT9l3ccMqh/P6kNlRKKR/q8yogRESKseU527jjg5mMn7OaI5rX5uF+XWh3SI0ieW4FhIhIMZSf77zx0xIe+mwOefnO387pwFVHt6R8OSuyGhQQIiLFzILszdw2YgY/LVrHsW3r848LOtO8btUir0MBISJSTOzKy+eFbxYwdGwGlVPK8fCFXbioRzPMiu6soSAFhIhIMTAzawO3DJ/O7BUbObPTIdzTtyMNa1ROak0KCBGRJNq+K4+hYzN44ZsF1K1Wkecu684ZnRrvu2ERUECIiCTJxAVruW3kDBau2cLFkeb85az21KpaIdll/T8FhIhIEdu0fRcPfjaHNyYuIbVuVd4Y2Itj2tZPdlm/oIAQESlCY2ev4o4PZrJ603YGHtuKP4Y0TUYiFM+qRERKmTWbd3DPR7P5aNpy2jWqwXOX96Br89rJLmuvQg0IM7sB+B1gwAvuPtTM/gmcC+wE5gO/cfecGG0XAZuAPCDX3SNh1ioiEgZ354OpWdz70Ww278jlplMP45oT21AxpfjfbSG0gDCzTkTDoSfRMPjczD4GxgC3B/edfgi4Hbi1kN2c5O5rwqpRRCRMWTnb+Ov7M/hybjbdUmvzUL8uHNaoaKbJSIQwzyDaAxPdfSuAmX0FXODuDxfY5kfgwhBrEBEpcvn5zusTF/PQZ3PId7jr3A5ccVTRTpORCGEGxEzg72ZWD9gGnAWk7bHNb4F3CmnvwGgzc+B5dx8WayMzGwQMAkhNTU1E3SIiByxz9WZuGzGdtMXrOe7Q+jxwfnKmyUiE0ALC3X8OupBGA1uAqUTHEwAws78CucAbheziWHfPMrOGwBgzm+PuX8d4nmHAMIBIJOIJPgwRkbjsysvn+a/m88S4TKpULM8jFx1Bv+5NkzZNRiKEOkjt7i8BLwGY2QPAsuDnq4BzgFPcPeaburtnBd9Xm9n7RMcyfhEQIiLJNmPZBm4ePo05KzdxdufG3N2nIw1qVEp2WQct7E8xNQze4FOBC4DeZnYGcAtwwu7xiRjtqgHl3H1T8PNpwL1h1ioisr+27cxj6Nh5vPDNAupXr8Tzl/fg9I6HJLushAn7OogRwRjELuBad88xs6eASkS7jQB+dPchZtYEeNHdzwIaAe8H61OAN93985BrFRGJ2w/z13L7yOksWruVAT2bc9uZ7alVpfhMk5EIYXcxHRdjWdtCtl1OdCAbd18AHBFmbSIiB2Lj9l3849M5vPXTElrUq8qbv+vF0W2K3zQZiaArqUVE4jRm9iru+GAG2Zt2MOj41tx06mFUqRjufaGTSQEhIrIP2Zt2cPdHs/hk+goOP6QGL1wRoUuz4j1NRiIoIERECuHujJycxX2fzGbrjjz+fNphDD6hDRXKF/9pMhJBASEiEsOy9Vv5y/sz+XpeNj1a1OGhfp1p27DkTJORCAoIEZEC8vKd135YxMNfzAXgnj4dubx3C8qVsGkyEkEBISISyFi1iVtHTGfykhxOOKwBfz+/E83qlMxpMhJBASEiZd7O3Hye+2o+T43PpGql8vzr10dwfreSPU1GIiggRKRMm7Y0h1tHTGfOyk2c0yU6TUb96iV/moxEUECISJm0bWce/xozl5e+XUiDGpV44YoIv+rQKNllFSsKCBEpc77PXMNtI2ewZN1WLumVym1nHk7NyqVrmoxEUECISJmxYdsuHvjkZ95JW0rLelV5e1Bvereul+yyii0FhIiUCZ/PXMmdH85k7ZadDD4hOk1G5Qqld5qMRFBAiEiptnrTdu4eNYtPZ6ykfeOavHTlkXRuVivZZZUICggRKZXcneHpy7j/k5/ZtiuPm09vx6DjW5eZaTISQQEhIqXO0nVb+cv7M/gmYw1HtqzDg/260KZB9WSXVeIoIESk1MjLd/79/SIe+WIu5Qzu69uRS3uVzWkyEiHUcy0zu8HMZprZLDO7MVhW18zGmFlG8L1OIW2vDLbJMLMrw6xTREq+eas20e/Z77nv49n0al2X0X88gcuPaqlwOAihnUGYWSfgd0BPYCfwuZl9DAwCxrn7g2Z2G3AbcOsebesCdwERwIF0Mxvl7uvDqldESqadufk882UmT0/IpHqlFIZe3JW+XZuU+WkyEiHMLqb2wER33wpgZl8BFwB9gRODbV4FvmSPgABOB8a4+7qg7RjgDOCtEOsVkRJmypL13DZiBnNXbaLPEU2469wO1NM0GQkTZkDMBP5uZvWAbUTvN50GNHL3FcE2K4FY17Y3BZYWeLwsWPYLZjaI6FkJqampialcRIq1rTtzeXT0PF7+biGNalTmpSsjnNJe02QkWmgB4e4/m9lDwGhgCzAVyNtjGzczP8jnGQYMA4hEIge1LxEp/r7NWMPt709n6bptXNY7lVvPOJwamiYjFKF+isndXwJeAjCzB4ieCawys8buvsLMGgOrYzTN4r/dUADNiHZFiUgZtWHrLu7/ZDbvpS+jVf1qvDOoN700TUaoQg0IM2vo7qvNLJXo+ENvoBVwJfBg8P3DGE2/AB4o8Amn04Dbw6xVRIqvz2as4M5Rs1i3ZSfXnNiGG045VNNkFIGwr4MYEYxB7AKudfccM3sQeNfMrgYWA78GMLMIMMTdB7r7OjO7D5gU7Ofe3QPWIlJ2rN64nTs/nMXns1bSsUlNXrnqSDo11TQZRcXcS0+3fSQS8bS0tGSXISIHyd15L20Z938ym+25+dx06mEMPK6VpskIgZmlu3sk1jpdSS0ixcqStVu5/f3pfJe5lp6t6vLgBZ1prWkykkIBISLFQl6+88p3C3lk9FxSypXj/vM6cUnPVF0JnUQKCBFJujkrN3LriBlMW5rDKYc35P7zO9G4VpVkl1XmKSBEJGl25Obx9IT5PDMhk5pVKvDEgG6c26WxpskoJhQQIpIU6YvXc+uI6WSu3sx5XZtw57kdqVutYrLLkgIUECJSpLbsyOWfX8zl1R8W0bhmZV656khOOrxhssuSGBQQIlJkvp6Xze0jZ5CVs40rjmrBLWccTvVKehsqrvTKiEjocrbu5L6Pf2bE5GW0blCN94YcxZEt6ya7LNkHBYSIhMbd+XTGSu4aNZP1W3dx7Ult+MPJmiajpFBAiEgoVm3czh0fzGTM7FV0alqTV3/bk45NNE1GSaKAEJGEcnfenrSUBz79mZ25+dx+5uFcfWwrUjRNRomjgBCRhFm0Zgu3j5zBDwvW0qtVXR7s14VW9asluyw5QAoIETlouXn5vPzdQh4dPY+K5cvxwPmd6X9kc02TUcIpIETkoPy8YiO3jpjO9GUbOLV9I+4/rxOH1Kqc7LIkARQQInJAduTm8dT4TJ79cj61qlTgqUu6cXZnTZNRmiggRGS/pS1ax60jpjM/ewsXdG/K387uQB1Nk1HqhH3L0ZuAgYADM4DfAGOAGsEmDYGf3P28GG3zgjYAS9y9T5i1isi+bd6Ryz8/n8N/flxMk1pV+PdvjuTEdpomo7QKLSDMrClwPdDB3beZ2btAf3c/rsA2I4h9T2qAbe7eNaz6RCR+O3PzGZ6+jCfHZ7By43au6N2CmzVNRqkX9qubAlQxs11AVWD57hVmVhM4mehZhYgUQ7vyosHw1PhMsnK20bV5bZ4c0I2IpskoE0ILCHfPMrNHgCXANmC0u48usMl5wDh331jILiqbWRqQCzzo7h/E2sjMBgGDAFJTUxNWv0hZtisvn5GTl/Hk+EyWrd/GEc1qcf/5nTjxsAYahC5DwuxiqgP0BVoBOcB7ZnaZu78ebDIAeHEvu2gRhExrYLyZzXD3+Xtu5O7DgGEAkUjEE3oQImVMbl4+I6dk8dT4TJas20qXZrW4t29HTmrXUMFQBoXZxXQqsNDdswHMbCRwNPC6mdUHegLnF9bY3bOC7wvM7EugG/CLgBCRg5ebl88HU5fz5PgMFq/dSqemNXnxigintFcwlGVhBsQSoLeZVSXaxXQKkBasuxD42N23x2oYnH1sdfcdQZgcAzwcYq0iZVJuXj4fBsGwaO1WOjapyQtXRDhVwSCEOwYx0cyGA5OJjiNMIegKAvoDDxbc3swiwBB3Hwi0B543s3ygHNExiNlh1SpS1uTlO6OmZfHkuEwWrNlC+8Y1ef7yHpzWoZGCQf6fuZeebvtIJOJpaWn73lCkjMrLdz6evpzHx2WwIHsLhx9SgxtPPZTTOhyieZPKKDNLd/dIrHX6ELNIGZCX73wyYwVPjMsgc/Vm2jWqwbOXduf0jgoGKZwCQqQUyy8QDBmrN3NYo+o8fUl3zuykYJB9U0CIlEL5+c5nM1fy+Lh5zFu1mbYNq/PkgOhkegoGiZcCQqQUyc93vpi1ksfHZTBn5SbaNKjG4/27ck6XJpRXMMh+UkCIlAL5+c7o2SsZOjYaDK0VDJIACgiREszdGT17FY+PzWD2io20ql+Nxy4+gj5HNFUwyEHbZ0CY2bnAJ+6eXwT1iEgc3J2xP69m6Nh5zFq+kZb1qvLoRUfQt2sTUsqXS3Z5UkrEcwZxMTA0mJr7ZXefE3JNIlIId2f8nNUMHZvBjKwNtKhXlUcuOoLzFAwSgn0GhLtfFkzNPQD4t5k58ArwlrtvCrtAEYkGw4S50WCYvmwDzetW4eELu3B+t6ZUUDBISOIag3D3jcG0GVWAG4lOsnezmT3h7k+GWaBIWebufDkvm6FjM5i2NIdmdarwUL/OXNC9mYJBQhfPGEQfojf1aQv8B+jp7quDSfhmAwoIkQRzd77OWMNjY+YxdWkOTWtX4R8XdKZf92ZUTFEwSNGI5wyiH/CYu39dcKG7bzWzq8MpS6Rscne+zYwGw+Ql0WB44PzOXNhDwSBFL56AuBtYsfuBmVUBGrn7IncfF1ZhImWJu/Nd5lqGjp1H2uL1NK5VmfvP68RFkWZUSimf7PKkjIonIN4jeqOf3fKCZUeGUpFIGeLu/DB/LUPHZvDTonUcUrMy9/XtyK+PbK5gkKSLJyBS3H3n7gfuvtPMKoZYk0iZ8MP8tTw2dh4/LVxHo5qVuKdPRy4+sjmVKygYpHiIJyCyzayPu48CMLO+wJpwyxIpvSYuiAbDjwvW0bBGJe46twMDeqYqGKTYiScghgBvmNlTgAFLgSvi2bmZ3QQMBByYQfTTUM8BJwAbgs2ucvepMdpeCdwRPLzf3V+N5zlFiqtJi9bx2Jh5fD9/LQ1qVOLOczpwSS8FgxRf8VwoN5/ovaWrB483x7NjM2sKXA90cPdtZvYu0VuNAtzs7sP30rYucBcQIRou6WY2yt3Xx/PcIsVJ2qJ1DB2bwbeZa6hfvRJ3nN2eS3u1oEpFBYMUb3FdKGdmZwMdgcq771fr7vfGuf8qZrYLqAosj7Ou04Ex7r4ueP4xwBnAW3G2F0m69MXrGTp2Ht9krKF+9Yr89az2XNZbwSAlRzwXyj1H9M39JOBF4ELgp321c/csM3sEWAJsA0a7+2gzuwT4u5ndCYwDbnP3HXs0b0q0K2u3ZcEykWJvypL1PDY2g6/nZVOvWkX+ctbhXNa7BVUravJkKVni+Y092t27mNl0d7/HzB4FPttXIzOrA/QFWgE5wHtmdhlwO7ASqAgMA24F4jkbKex5BgGDAFJTUw90NyIHberSHIaOnceXc7OpW60it515OJf3bkG1SgoGKZni+c3dHnzfamZNgLVA4zjanQosdPdsADMbSTRsXg/W7zCzV4A/x2ibBZxY4HEz4MtYT+Luw4gGDZFIxOOoSyShpi/L4bEx85gwN5vaVStwyxntuPKolgoGKfHi+Q3+yMxqA/8EJhMdNH4hjnZLiA5uVyXaxXQKkGZmjd19hUUHM84DZsZo+wXwQHAWAnAa0TMPkWJjxrINDB07j3FzVlO7agVuPr0dVx7dkuoKBikl9vqbbGblgHHungOMMLOPgcruvmFv7QDcfWIwA+xkIBeYQvQv/c/MrAHRj8xOJfoxWswsAgxx94Huvs7M7gMmBbu7d/eAtUiyzczawNCxGYz9eRW1qlTgz6cdxpVHt6RG5QrJLk0kocx9770yZjbF3bsVUT0HJRKJeFpaWrLLkFJq1vINPD42g9GzV1GzcgoDj2vNVce0pKaCQUowM0t390isdfGcC48zs37ASN9XmoiUQj+v2MjQsfP4YtYqalRO4cZTD+U3x7SiVhUFg5Ru8QTEYOCPQK6ZbSfaNeTuXjPUykSSbM7KjTw+NoPPZq6kRqUUrj/lUK4+VsEgZUc8V1LXKIpCRIqLeas28fjYDD6ZsYLqlVK4/uS2XH1sa2pVVTBI2RLPhXLHx1q+5w2EREq6jFWbeHxcNBiqVijPdSe1ZeBxrahdVZMXS9kUTxfTzQV+rgz0BNKBk0OpSKSIZa7exOPjMvl4+nKqVijPNSe04XfHtaZONQWDlG3xdDGdW/CxmTUHhoZWkUgRmZ+9mSfGZTBq2nKqVCjP4OPbMOj41tRVMIgAcU7Wt4dlQPtEFyJSVBZkb+bJ8Zl8ODWLSinlGXR8awYd15p61SsluzSRYiWeMYgniV49DVAO6Er04jeREmXhmi08OT6DD6ZkUTGlHAOPa82g41tTX8EgElM8ZxAFrzzLBd5y9+9Cqkck4Rav3cIT4zL5YGoWFcobvz2mFYNPaEODGgoGkb2JJyCGA9vdPQ/AzMqbWVV33xpuaSIHZ8narTw5PoORU7JIKWdceVRLhpzYmoY1Kie7NJESIa4rqYnOzLr7TnJVgNHA0WEVJXIwlq7bylPjMxkxeRnlyhlXHNWCa05oQ8OaCgaR/RFPQFQueJtRd98czNAqUqwsXbeVpydkMjw9GgyX9W7BNSe2oZGCQeSAxBMQW8ysu7tPBjCzHkSn7xYpFrJytvHU+EyGpy/FMC7plcrvT2zLIbUUDCIHI56AuJHo3eCWE52H6RDg4lCrEonD8pxtPD0hk3fTosHQ/8hUfn9SGxrXqpLs0kRKhXgulJtkZocD7YJFc919V7hliRRuxYZtPDNhPu9MWorj/DrSnGtPakuT2goGkUSK5zqIa4E33H1m8LiOmQ1w92dCr06kgJUbtvPMl5m8/dNS8t25KNKca09qQ7M6GhITCUM8XUy/c/endz9w9/Vm9jtAASFFYtXG7Tz75Xze/GkJ+fnOhT2ace1JbWleV8EgEqZ4AqK8mdnumwWZWXkgrslqzOwmYCDRK7FnAL8BXgIiwC7gJ2BwrC4rM8sL2gAscfc+8TynlB6rN27n2a/m8+bEJeTmOxd2b8Z1JysYRIpKPAHxOfCOmT0fPB4MfLavRmbWFLge6ODu28zsXaA/8AZwWbDZm0QD5NkYu9jm7l3jqE9KGXfnxW8W8sjoueTmOxd0a8ofTj6U1HoKBpGiFE9A3AoMAoYEj6cT/SRTvPuvYma7gKrAcncfvXulmf0ENIu/XCnttu7M5Zbh0/l4+gp+1aERfz2rPS3rV0t2WSJlUrl9beDu+cBEYBHRe0GcDPwcR7ss4BFgCbAC2LBHOFQALid6hhJLZTNLM7Mfzey8wp7HzAYF26VlZ2fvqywpxpas3coFz3zPJzNWcMsZ7Rh2eQ+Fg0gSFXoGYWaHAQOCrzXAOwDuflI8OzazOkBfoBWQQ/Raisvc/fVgk2eAr939m0J20cLds8ysNTDezGa4+/w9N3L3YcAwgEgk4nuul5Lh63nZ/OGtKbg7r1x1JCe2a5jskkTKvL2dQcwherZwjrsf6+5PAnn7se9TgYXunh0MQo8kmL/JzO4CGgB/LKxxcAaCuy8AvgS67cdzSwnh7jz75XyueuUnGteqzEd/OFbhIFJM7C0gLiDaNTTBzF4ws1OIXkkdryVAbzOramYGnAL8bGYDgdOBAUH31S8E11pUCn6uDxwDzN6P55YSYMuOXK57cwoPfT6HMzs3ZuTvj6ZFPXUpiRQXhXYxufsHwAdmVo1oV9GNQEMzexZ4v+B4QiHtJ5rZcKI3F8oFphDtCtoCLAZ+iOYGI939XjOLAEPcfSDRO9Y9b2b5REPsQXdXQJQii9duYfBr6cxbtYnbzjycwce3Jvh9EJFiwoLLG+LbODqucBFwsbufElpVBygSiXhaWtq+N5Sk+nLuaq5/awpmxpMDunH8YQ2SXZJImWVm6e4eibVuv+5J7e7riZ4FDEtEYVK2uDvPfDmfR0bPpV2jGgy7PKJrG0SKsf0KCJEDtWVHLjcPn8anM1Zy7hFNeKhfZ6pW1K+fSHGm/6ESuoVrtjD4tTQyV2/mr2e1Z+BxrTTeIFICKCAkVBOC8Yby5Yz//LYXxx5aP9kliUicFBASCnfn6QmZPDpmHu0Pqcnzl/fQJHsiJYwCQhJu845c/vzuND6ftZI+RzThoX5dqFKxfLLLEpH9pICQhFq4ZguD/pPG/OzN3HF2e64+VuMNIiWVAkISZvycVdzw9lRSyhmvX92Lo9tqvEGkJFNAyEHLz3eempDJY2Pn0aFxdLxBtwEVKfkUEHJQNm3fxZ/encbo2as4r2sT/nGBxhtESgsFhByw+dmbGfSfNBat3cqd53TgN8e01HiDSCmigJADMnb2Km56ZyoVUsrx2tU9ObqNxhtEShsFhOyX/HznifEZDB2bQaemNXn+8ghNa1dJdlkiEgIFhMRt4/Zd/PGdaYz9eRUXdG/KA+d3pnIFjTeIlFYKCIlL5urNDHotjcVrt3LXuR246miNN4iUdgoI2afRs1byx3enUSmlHG8M7EXv1vWSXZKIFIG93XL0oJnZTWY2y8xmmtlbZlbZzFqZ2UQzyzSzd8ysYiFtbw+2mWtmp4dZp8SWn+/8a8w8Br2WTusG1fjoD8cqHETKkNACwsyaAtcDEXfvBJQH+gMPAY+5e1tgPXB1jLYdgm07AmcAz5iZOruL0Mbtu/jdf9J4YlwG/bo3493BR9FEg9EiZUqoZxBEu7CqmFkKUBVYAZwMDA/WvwqcF6NdX+Btd9/h7guBTKBnyLVKIGPVJs576ju+mpfNvX078shFXTQYLVIGhRYQ7p4FPAIsIRoMG4B0IMfdc4PNlgFNYzRvCiwt8Liw7TCzQWaWZmZp2dnZiSq/zPpi1krOe/o7Nm7fxRsDe3HFURqMFimrwuxiqkP0TKAV0ASoRrS7KKHcfZi7R9w90qBBg0TvvszIz3ceHT2Xwa+l07ZhdT76w7H00niDSJkW5qeYTgUWuns2gJmNBI4BaptZSnAW0QzIitE2C2he4HFh20kCbNi2ixvfnsKEudlc1KMZ953XSV1KIhLqGMQSoLeZVbVoH8UpwGxgAnBhsM2VwIcx2o4C+ptZJTNrBRwK/BRirWXWvFWb6PvUt3yTsYb7+nbk4Qs13iAiUaGdQbj7RDMbDkwGcoEpwDDgE+BtM7s/WPYSgJn1IfqJpzvdfZaZvUs0UHKBa909L6xay6rPZ67gT+9Oo0rFFN4a1JsjW9ZNdkkiUoyYuye7hoSJRCKelpaW7DKKvbx8519j5vL0hPl0bV6b5y7rwSG1Kie7LBFJAjNLd/dIrHW6krqM2bB1Fze8M4Uv52bT/8jm3NO3I5VS1KUkIr+kgChD5q7cxKDX0lies42/n9+JS3qm6iOsIlIoBUQZ8emMFfz5vWlUq5TC24N606OFxhtEZO8UEKVcXr7zyOi5PPvlfLqlRscbGtXUeIOI7JsCohTL2bqT69+eytfzshnQM5W7+3TQeIOIxE0BUUrNWbmRQf9JZ8WGbTxwfmcu6ZWa7JJEpIRRQJRCH09fzs3vTadG5RTeHnQUPVrUSXZJIlICKSBKkbx85+Ev5vD8Vwvo0aIOz17anYYabxCRA6SAKCVytu7kD29N4ZuMNVzaK5W7zu1IxZSwZ3MXkdJMAVEKzF6+kcGvp7Fqww4evKAz/XtqvEFEDp4CooQbNW05twyfRq0qFXhncG+6pWq8QUQSQwFRQuXm5fPwF3MZ9vUCIi3q8Mxl3WlYQ+MNIpI4CogSaP2W6HjDt5lruLx3C/52TgeNN4hIwikgSphZyzcw+LV0Vm/cwcP9uvDrI5vvu5GIyAFQQJQgH07N4tYR06ldpSLvDjmKrs1rJ7skESnFFBAlQG5ePg9+NocXv0KxH1EAAAz4SURBVF1Iz5Z1efrS7jSoUSnZZYlIKRdaQJhZO+CdAotaA3cCRwHtgmW1gRx37xqj/SJgE5AH5BZ2Q4vSbt2WnVz35mS+n7+WK49qwR3ndKBCeY03iEj4wrzl6FygK4CZlQeygPfdfejubczsUWDDXnZzkruvCavG4m5mVnS8IXvzDv55YRcuimi8QUSKTlF1MZ0CzHf3xbsXWPRONb8GTi6iGkqU96cs47YRM6hbrSLDhxxFl2YabxCRolVUAdEfeGuPZccBq9w9o5A2Dow2Mweed/dhYRZYXOTm5fPAp3N4+buF9GxVl2cu7U796hpvEJGiF3pAmFlFoA9w+x6rBvDL0CjoWHfPMrOGwBgzm+PuX8fY/yBgEEBqasmeYmLt5h1c++ZkflywjquObslfz26v8QYRSZqiOIM4E5js7qt2LzCzFOACoEdhjdw9K/i+2szeB3oCvwiI4MxiGEAkEvHEll50ZizbwODX0li7ZSePXnQE/Xo0S3ZJIlLGFcWfp7HOFE4F5rj7slgNzKyamdXY/TNwGjAz1CqTaOTkZVz43PcADB9ytMJBRIqFUM8ggjf3XwGD91j1izEJM2sCvOjuZwGNgPej49ikAG+6++dh1poMu/Ly+fsnP/Pv7xfRu3Vdnr6kO/U03iAixUSoAeHuW4B6MZZfFWPZcuCs4OcFwBFh1pZsazbv4No3JjNx4TquPrYVt595OCkabxCRYkRXUifB9GU5DH4tnXVbdvLYxUdwfjd1KYlI8aOAKGLvpS3lrx/MpEH1Soy45mg6Na2V7JJERGJSQBSRXXn53P/xbF79YTFHt6nHU5d0p261iskuS0SkUAqIIpC9KTre8NOidQw8thW3abxBREoABUTIpi7NYchr6eRs28nj/bvSt2vTZJckIhIXBUSI3p20lDs+mEmDGtHxho5NNN4gIiWHAiIEO3Pzue/j2bz242KOaVuPJwdovEFESh4FRIKt3rSda9+YzKRF6xl8fGtuPr2dxhtEpERSQCTQlCXrGfJ6Ohu27eKJAd3oc0STZJckInLAFBAJ8s6kJfztg1k0qlWJkdccQ4cmNZNdkojIQVFAHKSdufnc89Es3pi4hOMOrc8T/btRR+MNIlIKKCAOwuqN27nmjcmkL17P4BNac8vph1O+nCW7LBGRhFBAHKD0xeu55vV0Nm3P5alLunFOF403iEjpooA4AG9OXMJdo2bSuFYVXv1tT9o31niDiJQ+Coj9sCM3j7tHzeatn5Zw/GENeKJ/V2pX1XiDiJROCog4rdq4nSGvpzNlSQ6/P7ENfzqtncYbRKRUU0DEIX3xOoa8PpktO3J55tLunNW5cbJLEhEJXWiX+JpZOzObWuBro5ndaGZ3m1lWgeVnFdL+DDOba2aZZnZbWHXujbvzxsTF9B/2I1Urluf93x+jcBCRMiO0Mwh3nwt0BTCz8kAW8D7wG+Axd3+ksLbB9k8TvZ/1MmCSmY1y99lh1bunHbl53PXhLN6etJQT2zXg8Yu7UatqhaJ6ehGRpCuqLqZTgPnuvtgsrn77nkBmcG9qzOxtoC9QJAGxckN0vGHq0hyuPakNf/yVxhtEpOwpqlnk+gNvFXh8nZlNN7OXzaxOjO2bAksLPF4WLPsFMxtkZmlmlpadnX3QhU5atI5znvyWeas28eyl3blZF7+JSBkVekCYWUWgD/BesOhZoA3R7qcVwKMHs393H+buEXePNGjQ4GD2w2s/LGLAsB+pUTmFD649hjM13iAiZVhRdDGdCUx291UAu78DmNkLwMcx2mQBzQs8bhYsC8X2XXnc+eFM3k1bxkntGjC0fzdqVdF4g4iUbUUREAMo0L1kZo3dfUXw8HxgZow2k4BDzawV0WDoD1wSRnEbtu7iild+YtrSHK4/uS03nnoY5dSlJCISbkCYWTWin0QaXGDxw2bWFXBg0e51ZtYEeNHdz3L3XDO7DvgCKA+87O6zwqixRuUUWtaryjUntOGMToeE8RQiIiWSuXuya0iYSCTiaWlpyS5DRKTEMLN0d4/EWqd7YYqISEwKCBERiUkBISIiMSkgREQkJgWEiIjEpIAQEZGYFBAiIhKTAkJERGIqVRfKmVk2sPgAm9cH1iSwnGQqLcdSWo4DdCzFUWk5Dji4Y2nh7jFnOi1VAXEwzCytsKsJS5rSciyl5ThAx1IclZbjgPCORV1MIiISkwJCRERiUkD817BkF5BApeVYSstxgI6lOCotxwEhHYvGIEREJCadQYiISEwKCBERialMBYSZNTezCWY228xmmdkNMbYxM3vCzDLNbLqZdU9GrXsT53GcaGYbzGxq8HVnMmrdFzOrbGY/mdm04FjuibFNJTN7J3hNJppZy6KvdN/iPJarzCy7wOsyMBm1xsPMypvZFDP7xX3jS8prsts+jqUkvSaLzGxGUOcv7o6W6PevorgndXGSC/zJ3SebWQ0g3czGuPvsAtucCRwafPUCng2+FyfxHAfAN+5+ThLq2x87gJPdfbOZVQC+NbPP3P3HAttcDax397Zm1h94CLg4GcXuQzzHAvCOu1+XhPr21w3Az0DNGOtKymuy296OBUrOawJwkrsXdlFcQt+/ytQZhLuvcPfJwc+biP7CNN1js77AfzzqR6C2mTUu4lL3Ks7jKBGCf+fNwcMKwdeen5zoC7wa/DwcOMXMrIhKjFucx1IimFkz4GzgxUI2KRGvCcR1LKVJQt+/ylRAFBScEncDJu6xqimwtMDjZRTjN9+9HAfAUUF3x2dm1rFIC9sPwen/VGA1MMbdC31N3D0X2ADUK9oq4xPHsQD0C07/h5tZ8yIuMV5DgVuA/ELWl5jXhH0fC5SM1wSif3CMNrN0MxsUY31C37/KZECYWXVgBHCju29Mdj0Hah/HMZnoHCtHAE8CHxR1ffFy9zx37wo0A3qaWadk13Sg4jiWj4CW7t4FGMN//wovNszsHGC1u6cnu5aDFeexFPvXpIBj3b070a6ka83s+DCfrMwFRNA3PAJ4w91HxtgkCyj4F0SzYFmxsq/jcPeNu7s73P1ToIKZ1S/iMveLu+cAE4Az9lj1/6+JmaUAtYC1RVvd/insWNx9rbvvCB6+CPQo6tricAzQx8wWAW8DJ5vZ63tsU1Jek30eSwl5TQBw96zg+2rgfaDnHpsk9P2rTAVE0Ef6EvCzu/+rkM1GAVcEnwboDWxw9xVFVmQc4jkOMztkd5+wmfUk+loXu//AZtbAzGoHP1cBfgXM2WOzUcCVwc8XAuO9GF7hGc+x7NEf3Ifo+FGx4u63u3szd28J9Cf6733ZHpuViNcknmMpCa8JgJlVCz6UgplVA04DZu6xWULfv8rap5iOAS4HZgT9xAB/AVIB3P054FPgLCAT2Ar8Jgl17ks8x3EhcI2Z5QLbgP7F8T8w0Bh41czKEw2xd939YzO7F0hz91FEw/A1M8sE1hH9j14cxXMs15tZH6KfRFsHXJW0avdTCX1NYiqhr0kj4P3g774U4E13/9zMhkA471+aakNERGIqU11MIiISPwWEiIjEpIAQEZGYFBAiIhKTAkJERGJSQEiZZmZe8MIpM0sJZvb8xayfB7Dv3TPqTjGzuWb2dXBl74Hur6WZXVLg8VVm9tTB1ilSGAWElHVbgE7BhW0QvbgtkVfOf+Pu3dy9HXA98JSZnXKA+2oJXLKvjUQSRQEhEr246Ozg5wHAW7tXmFlPM/shOAv43szaBctvMrOXg587m9lMM6u6tydx96nAvcB1QbsGZjbCzCYFX8cEy+82s9eC580ws98Fu3gQOM6i9wK4KVjWxMw+D7Z7ODH/HCJRCgiR6Bw9/c2sMtCF/50Zdw5wnLt3A+4EHgiWPw60NbPzgVeAwe6+NY7nmgwcXmAfj7n7kUA//nc66i7AycBRwJ1m1gS4jegZSVd3fyzYrivR+zB0Bi4u5jORSglT1qbaEPkFd58eTJs+gOjZREG1iE6fcSjRqZYrBG3yzewqYDrwvLt/F+fTFbxnwqlAB/vvbRRqBjP0Anzo7tuAbWY2geikbDkx9jfO3TcAmNlsoAX/O92zyAFTQIhEjQIeAU7kf+9rcB8wwd3PD0LkywLrDgU2A03243m68d/J4MoBvd19e8ENgsDYcw6cwubE2VHg5zz0f1oSSF1MIlEvA/e4+4w9ltfiv4PWV+1eaGa1gCeA44F6Znbhvp7AzLoAfwOeDhaNBv5QYH3XApv3teg9rusRDa1JwCagRvyHJHJwFBAigLsvc/cnYqx6GPiHmU3hf/86fwx42t3nEb0/84Nm1jBG++N2f8yVaDBc7+7jgnXXAxGL3slsNjCkQLvpRO8n8SNwn7svD5blWfQugTchEjLN5ipSzJjZ3cBmd38k2bVI2aYzCBERiUlnECIiEpPOIEREJCYFhIiIxKSAEBGRmBQQIiISkwJCRERi+j9gztZ7kJ8JBgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKnuaJ_S1dew",
        "outputId": "e85b2959-38e7-4981-8dc9-551aa7fde597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "#with sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "x = [1,2,3,4,5]\n",
        "y = []\n",
        "for i in range (1,6):\n",
        "    accuracy = 0\n",
        "    for j in range(1,11):\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "        training_data,testing_data=train_test_split(dataset)\n",
        "        x_train = training_data.iloc[0:230,1:10]\n",
        "        y_train = training_data.iloc[0:230,0:1]\n",
        "        x_test  = testing_data.iloc[0:230,1:10]\n",
        "        y_test  = testing_data.iloc[0:230,0:1]\n",
        "        clf = tree.DecisionTreeClassifier(criterion = \"entropy\", max_depth = i)\n",
        "        clf.fit(x_train,y_train)\n",
        "        accuracy = accuracy + clf.score(x_train,y_train)\n",
        "    y.append(accuracy/10)\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0da626465113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m230\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '60-69'"
          ]
        }
      ]
    }
  ]
}