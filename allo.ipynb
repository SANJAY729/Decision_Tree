{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SANJAY729/Decision_Tree/blob/master/allo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnYS2roA3MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cd14f61-43bb-4059-ee6a-12fdeabae367"
      },
      "source": [
        "print(\"allo testing dhar\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allo testing dhar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwmH98velv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hello123"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBCiLs7wXbBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aitjxFhv7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Pog Code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "################### \n",
        "    \n",
        "###################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,max_depth,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    # print(features)\n",
        "    # print(data[target_attribute_name])\n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) == 0 or max_depth == 0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,max_depth-1,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree,default = 1):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    #1.\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    return (np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100\n",
        "    #print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PANbgbMCUBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the dataset and define the feature as well as the target datasets / columns#\n",
        "dataset = pd.read_csv('breast-cancer.csv',\n",
        "                      names=['class','age','menopause','tumor-size','inv-nodes','node-caps','def-malig','breast','breast-quad','irradiat'])#Import all columns omitting the fist which consists the names of the animals\n",
        "# Class: no-recurrence-events, recurrence-events\n",
        "# 2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
        "# 3. menopause: lt40, ge40, premeno.\n",
        "# 4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
        "# 5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
        "# 6. node-caps: yes, no.\n",
        "# 7. deg-malig: 1, 2, 3.\n",
        "# 8. breast: left, right.\n",
        "# 9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
        "# 10. irradiat: yes, no.\n",
        "\n",
        "\n",
        "#We drop the animal names since this is not a good feature to split the data on\n",
        "#dataset=dataset.drop('animal_name',axis=1)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j9scP8RCbVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:230].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)#shuffle\n",
        "# training_data = train_test_split(dataset)[0]\n",
        "# testing_data = train_test_split(dataset)[1] \n",
        "training_data,testing_data=train_test_split(dataset)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uPcELuCG4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "05b51cac-94d9-43d6-ced6-b29b328a5f8d"
      },
      "source": [
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "x = [1,2,3,4,5]\n",
        "y = [] \n",
        "for i in range(1,6):\n",
        "  tree = ID3(training_data,training_data,training_data.columns[1:],i)\n",
        "  y.append(test(testing_data,tree))\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}