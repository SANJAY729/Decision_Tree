{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SANJAY729/Decision_Tree/blob/master/allo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnYS2roA3MI",
        "outputId": "ad5c7a0c-e717-45e5-bb65-7f4ea193e63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"allo testing dhar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allo testing dhar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwmH98velv8"
      },
      "source": [
        "#hello123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBCiLs7wXbBP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3aitjxFhv7p"
      },
      "source": [
        "#The Pog Code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def entropy(target_col):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    The only parameter of this function is the target_col parameter which specifies the target column\n",
        "    \"\"\"\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "################### \n",
        "    \n",
        "###################\n",
        "\n",
        "\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
        "    3. target_name = the name of the target feature. The default for this example is \"class\"\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain\n",
        "       \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "def ID3(data,originaldata,features,max_depth,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. target_attribute_name = the name of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    # print(features)\n",
        "    # print(data[target_attribute_name])\n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data) == 0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) == 0 or max_depth == 0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{},\"recurrence-events\":0,\"no-recurrence-events\":0}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,max_depth-1,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    \n",
        "                \n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "def predict(query,tree):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "\n",
        "\n",
        "    We do this also in a recursive manner. That is, we wander down the tree and check if we have reached a leaf or if we are still in a sub tree. \n",
        "    Since this is a important step to understand, the single steps are extensively commented below.\n",
        "\n",
        "    1.Check for every feature in the query instance if this feature is existing in the tree.keys() for the first call, \n",
        "    tree.keys() only contains the value for the root node \n",
        "    --> if this value is not existing, we can not make a prediction and have to \n",
        "    return the default value which is the majority value of the target feature\n",
        "\n",
        "    2. First of all we have to take care of a important fact: Since we train our model with a database A and then show our model\n",
        "    a unseen query it may happen that the feature values of these query are not existing in our tree model because non of the\n",
        "    training instances has had such a value for this specific feature. \n",
        "    For instance imagine the situation where your model has only seen animals with one to four\n",
        "    legs - The \"legs\" node in your model will only have four outgoing branches (from one to four). If you now show your model\n",
        "    a new instance (animal) which has for the legs feature the vale 5, you have to tell your model what to do in such a \n",
        "    situation because otherwise there is no classification possible because in the classification step you try to \n",
        "    run down the outgoing branch with the value 5 but there is no such a branch. Hence: Error and no Classification!\n",
        "    We can address this issue with a classification value of for instance (999) which tells us that there is no classification\n",
        "    possible or we assign the most frequent target feature value of our dataset used to train the model. Or, in for instance \n",
        "    medical application we can return the most worse case - just to make sure... \n",
        "    We can also return the most frequent value of the direct parent node. To make a long story short, we have to tell the model \n",
        "    what to do in this situation.\n",
        "    In our example, since we are dealing with animal species where a false classification is not that critical, we will assign\n",
        "    the value 1 which is the value for the mammal species (for convenience).\n",
        "\n",
        "    3. Address the key in the tree which fits the value for key --> Note that key == the features in the query. \n",
        "    Because we want the tree to predict the value which is hidden under the key value (imagine you have a drawn tree model on \n",
        "    the table in front of you and you have a query instance for which you want to predict the target feature \n",
        "    - What are you doing? - Correct:\n",
        "    You start at the root node and wander down the tree comparing your query to the node values. Hence you want to have the\n",
        "    value which is hidden under the current node. If this is a leaf, perfect, otherwise you wander the tree deeper until you\n",
        "    get to a leaf node. \n",
        "    Though, you want to have this \"something\" [either leaf or sub_tree] which is hidden under the current node\n",
        "    and hence we must address the node in the tree which == the key value from our query instance. \n",
        "    This is done with tree[keys]. Next you want to run down the branch of this node which is equal to the value given \"behind\"\n",
        "    the key value of your query instance e.g. if you find \"legs\" == to tree.keys() that is, for the first run == the root node.\n",
        "    You want to run deeper and therefore you have to address the branch at your node whose value is == to the value behind key.\n",
        "    This is done with query[key] e.g. query[key] == query['legs'] == 0 --> Therewith we run down the branch of the node with the\n",
        "    value 0. Summarized, in this step we want to address the node which is hidden behind a specific branch of the root node (in the first run)\n",
        "    this is done with: result = [key][query[key]]\n",
        "\n",
        "    4. As said in the 2. step, we run down the tree along nodes and branches until we get to a leaf node.\n",
        "    That is, if result = tree[key][query[key]] returns another tree object (we have represented this by a dict object --> \n",
        "    that is if result is a dict object) we know that we have not arrived at a root node and have to run deeper the tree. \n",
        "    Okay... Look at your drawn tree in front of you... what are you doing?...well, you run down the next branch... \n",
        "    exactly as we have done it above with the slight difference that we already have passed a node and therewith \n",
        "    have to run only a fraction of the tree --> You clever guy! That \"fraction of the tree\" is exactly what we have stored\n",
        "    under 'result'.\n",
        "    So we simply call our predict method using the same query instance (we do not have to drop any features from the query\n",
        "    instance since for instance the feature for the root node will not be available in any of the deeper sub_trees and hence \n",
        "    we will simply not find that feature) as well as the \"reduced / sub_tree\" stored in result.\n",
        "\n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    default = \"no-recurrence-events\"\n",
        "    if (tree[\"recurrence-events\"]>tree[\"no-recurrence-events\"]):\n",
        "        default = \"recurrence-events\"\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "Check the accuracy of our prediction.\n",
        "The train_test_split function takes the dataset as parameter which should be divided into\n",
        "a training and a testing set. The test function takes two parameters, which are the testing data as well as the tree model.\n",
        "\"\"\"\n",
        "###################\n",
        "\n",
        "###################\n",
        "\n",
        "\n",
        "\n",
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,1:].to_dict(orient = \"records\")\n",
        "    \n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree)\n",
        "    return (np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100\n",
        "    #print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
        "    \n",
        "def majorityclassutil(query,tree):\n",
        "    if (query[\"class\"]==\"no-recurrence-events\"):\n",
        "        tree[\"no-recurrence-events\"] = tree[\"no-recurrence-events\"] + 1\n",
        "    else:\n",
        "        tree[\"recurrence-events\"] = tree[\"recurrence-events\"] + 1\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return\n",
        "            result = tree[key][query[key]]\n",
        "            if isinstance(result,dict):\n",
        "                return majorityclassutil(query,result)\n",
        "            else:\n",
        "                return\n",
        "def majorityclasses(data,tree):\n",
        "     queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "     for i in range(len(data)):\n",
        "        majorityclassutil(queries[i],tree)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PANbgbMCUBZ"
      },
      "source": [
        "#Import the dataset and define the feature as well as the target datasets / columns#\n",
        "dataset_init = pd.read_csv('breast-cancer.csv',\n",
        "                      names=['class','age','menopause','tumor-size','inv-nodes','node-caps','def-malig','breast','breast-quad','irradiat'])#Import all columns omitting the fist which consists the names of the animals\n",
        "# Class: no-recurrence-events, recurrence-events\n",
        "# 2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
        "# 3. menopause: lt40, ge40, premeno.\n",
        "# 4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.\n",
        "# 5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.\n",
        "# 6. node-caps: yes, no.\n",
        "# 7. deg-malig: 1, 2, 3.\n",
        "# 8. breast: left, right.\n",
        "# 9. breast-quad: left-up, left-low, right-up, right-low, central.\n",
        "# 10. irradiat: yes, no.\n",
        "\n",
        "\n",
        "dataset_ = dataset_init[dataset_init['node-caps']!='?']\n",
        "dataset = dataset_[dataset_['breast-quad']!='?']\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j9scP8RCbVe"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:230].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,testing_data"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-uPcELuCG4Z",
        "outputId": "524d50af-6a0d-4907-a9ba-2a1da2ad3e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "\"\"\"\n",
        "Train the tree, Print the tree and predict the accuracy\n",
        "\"\"\"\n",
        "\n",
        "x = [1,2,3,4,5,6,7,8,9]\n",
        "y = []\n",
        "for i in range (1,10):\n",
        "    accuracy = 0\n",
        "    for j in range(1,11):\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "        training_data,testing_data=train_test_split(dataset)\n",
        "        tree = ID3(training_data,training_data,training_data.columns[1:],i)\n",
        "        majorityclasses(training_data,tree)\n",
        "        accuracy = accuracy + test(testing_data,tree)\n",
        "    y.append(accuracy/10)\n",
        "plt.plot(x,y)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.show\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl6UzIF4qQOD"
      },
      "source": [
        "#pruning\n",
        "def train_validation_test_split(dataset):\n",
        "    training_data = dataset.iloc[:180].reset_index(drop=True)\n",
        "    validation_data = dataset.iloc[180:230].reset_index(drop=True)\n",
        "    testing_data = dataset.iloc[230:].reset_index(drop=True)\n",
        "    return training_data,validation_data,testing_data\n",
        "\n",
        "# max_depth  = 3\n",
        "# pd.options.display.max_columns = None\n",
        "# pd.options.display.max_rows = None\n",
        "# dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "# training_data,validation_data,testing_data = train_validation_test_split(dataset)\n",
        "# tree = ID3(training_data,training_data,training_data.columns[1:],max_depth)\n",
        "# majorityclasses(training_data,tree)\n",
        "# initial_accuracy = test(validation_data,tree)\n",
        "# #print(validation_data)\n",
        "# print(initial_accuracy)\n",
        "# pprint(tree)\n",
        "# #def post_pruning(tree,initial_accuracy,validation_data):\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QYubA4imTQQ"
      },
      "source": [
        "def possiblevalues(keys):\n",
        "    feature = \"\"\n",
        "    values = []\n",
        "    for key in keys:\n",
        "        if (key == 'no-recurrence-events'):\n",
        "            continue\n",
        "        elif (key == 'recurrence-events'):\n",
        "            continue\n",
        "        else :\n",
        "            feature = key\n",
        "            if (feature == 'age'):\n",
        "                values = ['10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-89', '90-99']\n",
        "            if (feature == 'menopause'):\n",
        "                values = ['lt40', 'ge40', 'premeno']\n",
        "            if (feature == 'tumor-size'):\n",
        "                values = ['0-4', '5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59']\n",
        "            if (feature == 'inv-nodes'):\n",
        "                values = ['0-2', '3-5', '6-8', '9-11', '12-14', '15-17', '18-20', '21-23', '24-26', '27-29', '30-32', '33-35', '36-39']\n",
        "            if (feature == 'node-caps'):\n",
        "                values = ['yes', 'no']\n",
        "            if (feature == 'deg-malig'):\n",
        "                values = ['1', '2', '3']\n",
        "            if (feature == 'breast'):\n",
        "                values = ['left', 'right']\n",
        "            if (feature == 'breast-quad'):\n",
        "                values = ['left-up', 'left-low', 'right-up', 'right-low', 'central']\n",
        "            if (feature == 'irradiat'):\n",
        "                values = ['yes', 'no']            \n",
        "    return feature,values\n",
        "\n",
        "def skip():\n",
        "  return\n",
        "def delete_node(tree,node,target):\n",
        "    if (node==tree):\n",
        "        return target\n",
        "    else:\n",
        "        if isinstance(tree, dict):\n",
        "            feature, values = possiblevalues(tree.keys())\n",
        "            for value in values:\n",
        "                try:\n",
        "                    tree[feature][value] = copy.deepcopy(delete_node(tree[feature][value],node,target))\n",
        "                except:\n",
        "                    skip()\n",
        "    return tree\n",
        "def post_pruning(tree,initial_accuracy,validation_data):\n",
        "    initial_tree = copy.deepcopy(tree)\n",
        "    best_tree = copy.deepcopy(tree)\n",
        "    current_tree = copy.deepcopy(tree)\n",
        "    best_accuracy = initial_accuracy\n",
        "    flag = 0\n",
        "    queue = []\n",
        "    node_removed=None\n",
        "    queue.append(tree)\n",
        "    while queue!=[]:\n",
        "        node = queue.pop(0)\n",
        "        if isinstance(node, dict):\n",
        "            feature, values = possiblevalues(node.keys())\n",
        "            for value in values:\n",
        "                try:\n",
        "                    queue.append(node[feature][value])\n",
        "                    \n",
        "                except:\n",
        "                    skip()\n",
        "            if node==initial_tree :\n",
        "                print('hello')\n",
        "                continue\n",
        "            \n",
        "            if node['no-recurrence-events']>node['recurrence-events']:\n",
        "                current_tree=copy.deepcopy(delete_node(initial_tree,node,'no-recurrence-events'))\n",
        "            else :\n",
        "                current_tree=copy.deepcopy(delete_node(initial_tree,node,'recurrence-events'))\n",
        "            initial_tree = copy.deepcopy(tree)\n",
        "            #print(current_tree)\n",
        "            current_accuracy = test(validation_data,current_tree)\n",
        "            if current_accuracy>best_accuracy:\n",
        "                flag = 1\n",
        "                best_tree = copy.deepcopy(current_tree)\n",
        "                node_removed = node\n",
        "                best_accuracy = current_accuracy    \n",
        "    return best_tree,node_removed,best_accuracy,flag"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_xtL_vSwZMp",
        "outputId": "7aea096c-d619-4de0-e8af-a48ad8442d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "max_depth  = 3\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "training_data,validation_data,testing_data = train_validation_test_split(dataset)\n",
        "tree = ID3(training_data,training_data,training_data.columns[1:],max_depth)\n",
        "majorityclasses(training_data,tree)\n",
        "initial_accuracy = test(validation_data,tree)\n",
        "#print(validation_data)\n",
        "print('validation data initial acc',initial_accuracy)\n",
        "#pprint(tree)\n",
        "ini_tree=copy.deepcopy(tree)\n",
        "best_tree,node_removed,best_accuracy,flag=post_pruning(tree,initial_accuracy,validation_data)\n",
        "print('validation data after 1 prune acc',best_accuracy)\n",
        "print('testing accuracy before prune',test(testing_data,ini_tree))\n",
        "print('testing accuracy after prune',test(testing_data,best_tree))\n",
        "#print(flag,best_accuracy)\n",
        "#pprint(node_removed)\n",
        "#pprint(best_tree)\n",
        "#pprint(ini_tree)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation data initial acc 74.0\n",
            "hello\n",
            "validation data after 1 prune acc 78.0\n",
            "testing accuracy before prune 78.72340425531915\n",
            "testing accuracy after prune 74.46808510638297\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}